# Model Configuration
MODEL_DIR=checkpoints/openaudio-s1-mini

# Fish Speech Installation Directory
# Update this path to point to your Fish Speech installation
FISH_SPEECH_DIR=C:\Users\VM02\Desktop\THESIS (SALAS)\SECOND PHASE ENV\final\fish-speech

# Device Configuration
# Options: auto, cuda, mps, cpu
# - auto: Automatically detect best device (recommended)
# - cuda: Force NVIDIA GPU (Windows/Linux with CUDA)
# - mps: Force Apple Silicon GPU (macOS M1/M2/M3/M4)
# - cpu: Force CPU (slowest, but works everywhere)
DEVICE=auto

# Optimization Settings
ENABLE_TORCH_COMPILE=False  # Enable torch.compile (experimental, may cause instability)
MIXED_PRECISION=auto  # auto, bf16, fp16, or fp32
QUANTIZATION=none  # none, int8, or 4bit

# Model Parameters
MAX_SEQ_LEN=1024  # Maximum sequence length
CHUNK_SIZE=8192  # Audio chunk size for processing
NUM_STREAMS=3  # Number of CUDA streams for parallel processing
CACHE_LIMIT=100  # Maximum number of cached items

# Server Configuration
HOST=0.0.0.0
PORT=8000
GRADIO_PORT=7860
API_URL=http://localhost:8000

# Performance Tuning
# Reduce memory fragmentation (uncomment if needed)
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Logging
LOG_LEVEL=info
