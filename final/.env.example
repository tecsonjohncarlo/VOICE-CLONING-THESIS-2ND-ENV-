# Model Configuration
MODEL_DIR=checkpoints/openaudio-s1-mini

# Fish Speech Installation Directory
# Update this path to point to your Fish Speech installation
FISH_SPEECH_DIR=C:\Users\VM02\Desktop\THESIS (SALAS)\SECOND PHASE ENV\voice cloning (FINAL)\scripts\fish-speech

# Device Configuration
# Options: auto, cuda, mps, cpu
#
# - auto: ðŸ¤– INTELLIGENT AUTO-SELECTION (recommended)
#   * Monitors GPU/CPU utilization in real-time
#   * Switches to CPU if GPU >85% utilized or >90% VRAM
#   * Switches to GPU if CPU >90% utilized and GPU available
#   * Optimizes based on system memory pressure
#   * Checks every 5 seconds, avoids thrashing
#
# - cuda: ðŸ”’ LOCK TO NVIDIA GPU
#   * Always uses GPU (requires PyTorch with CUDA support)
#   * Won't auto-switch even if GPU is overloaded
#   * Falls back to CPU if CUDA not available
#
# - mps: ðŸ”’ LOCK TO APPLE SILICON GPU
#   * Always uses Apple GPU (macOS M1/M2/M3/M4 only)
#   * Won't auto-switch even if GPU is overloaded
#   * Falls back to CPU if MPS not available
#
# - cpu: ðŸ”’ LOCK TO CPU
#   * Always uses CPU, never uses GPU
#   * Good for power saving, debugging, or freeing GPU
#
# THREE WAYS TO CONTROL DEVICE:
# 1. Permanent: Set DEVICE=cpu/cuda/mps here (locks device)
# 2. Temporary: Use "Force CPU Mode" checkbox in Gradio UI
# 3. Smart (default): Set DEVICE=auto for intelligent optimization
DEVICE=auto

# Optimization Settings
ENABLE_TORCH_COMPILE=False  # Enable torch.compile (experimental, may cause instability)
MIXED_PRECISION=auto  # auto, bf16, fp16, or fp32
QUANTIZATION=none  # none, int8, or 4bit

# Model Parameters
MAX_SEQ_LEN=1024  # Maximum sequence length
CHUNK_SIZE=8192  # Audio chunk size for processing
NUM_STREAMS=3  # Number of CUDA streams for parallel processing
CACHE_LIMIT=100  # Maximum number of cached items

# Server Configuration
HOST=0.0.0.0
PORT=8000
GRADIO_PORT=7860
API_URL=http://localhost:8000

# Performance Tuning
# CRITICAL FIX: Reduce memory fragmentation for 4GB GPUs
# This prevents memory overflow to system RAM which causes 10-20x slowdown
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Logging
LOG_LEVEL=info
