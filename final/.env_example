# ============================================================================
# FISH SPEECH TTS - ENVIRONMENT CONFIGURATION
# ============================================================================
# Copy this file to .env and configure for your hardware
# The system will auto-detect most settings, but you can override them here

# ============================================================================
# DEVICE CONFIGURATION
# ============================================================================

# Device selection (auto-detect by default)
# Options: auto, cuda, mps, cpu
# - auto: Automatically detect best available device (recommended)
# - cuda: Force NVIDIA GPU (requires CUDA installation)
# - mps: Force Apple Silicon GPU (macOS only)
# - cpu: Force CPU-only mode (slowest, but works everywhere)
DEVICE=auto

# For multi-GPU systems: Specify which GPU to use (0-indexed)
# Leave empty to use GPU 0 (default)
# Example: CUDA_VISIBLE_DEVICES=1 (use second GPU)
# CUDA_VISIBLE_DEVICES=0

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Mixed precision mode
# Options: fp16, bf16, fp32
# - fp16: Half precision (default, fastest, works on most GPUs)
# - bf16: Brain float 16 (Ampere+ GPUs: RTX 3000/4000, A100)
# - fp32: Full precision (slowest, highest quality, more memory)
MIXED_PRECISION=fp16

# Quantization (reduces memory usage, slight quality loss)
# Options: none, int8
# - none: No quantization (high-end GPUs with 12GB+ VRAM)
# - int8: INT8 quantization (mid/low-end GPUs, saves ~40% memory)
# Note: Auto-configured based on GPU memory, override if needed
# QUANTIZATION=auto

# torch.compile optimization (20-30% speedup on supported platforms)
# Options: auto, true, false
# - auto: Enable on Linux/WSL2 with NVIDIA GPU (recommended)
# - true: Force enable (may cause issues on macOS/Windows)
# - false: Disable (safer for macOS/Windows)
ENABLE_TORCH_COMPILE=auto

# Force torch.compile even on unsupported platforms (advanced users only)
# FORCE_TORCH_COMPILE=0

# Number of CPU threads (auto-detect by default)
# Set to number of physical cores for best performance
# OMP_NUM_THREADS=auto
# MKL_NUM_THREADS=auto

# ============================================================================
# MEMORY MANAGEMENT
# ============================================================================

# Memory budget (GB) - prevents OOM errors
# Leave empty for auto-detection based on available RAM
# Example: MEMORY_BUDGET_GB=6 (for 8GB RAM systems)
# MEMORY_BUDGET_GB=auto

# Cache settings
# CACHE_LIMIT=100

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Model path (relative to project root)
MODEL_PATH=checkpoints/openaudio-s1-mini

# Maximum text length (characters)
# Auto-configured based on GPU memory:
# - High-end GPU (16GB+): 2000 chars
# - Mid-range GPU (8-16GB): 1000 chars
# - Low-end GPU/CPU (<8GB): 600 chars
# MAX_TEXT_LENGTH=auto

# ============================================================================
# LOGGING & MONITORING
# ============================================================================

# Log level
# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Performance monitoring output directory
METRICS_OUTPUT_DIR=./metrics

# Enable CSV logging for performance analysis
ENABLE_MONITORING=true

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# API server host and port
HOST=0.0.0.0
PORT=8000

# Number of worker processes (Uvicorn)
# WORKERS=1

# ============================================================================
# CUDA INSTALLATION GUIDE (for NVIDIA GPU users)
# ============================================================================
#
# IMPORTANT: You need CUDA Toolkit installed to use NVIDIA GPUs
#
# 1. Check your GPU compatibility:
#    - Run: nvidia-smi
#    - Note your CUDA version (e.g., "CUDA Version: 12.1")
#
# 2. Install CUDA Toolkit:
#
#    WINDOWS:
#    --------
#    a) Download from: https://developer.nvidia.com/cuda-downloads
#    b) Select: Windows > x86_64 > 10/11 > exe (local)
#    c) Install with default settings
#    d) Verify: nvcc --version
#
#    LINUX (Ubuntu/Debian):
#    ----------------------
#    # For CUDA 12.1 (recommended for RTX 3000/4000 series)
#    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
#    sudo dpkg -i cuda-keyring_1.0-1_all.deb
#    sudo apt-get update
#    sudo apt-get -y install cuda-toolkit-12-1
#    
#    # For CUDA 11.8 (for older GPUs: GTX 1000/RTX 2000 series)
#    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
#    sudo dpkg -i cuda-keyring_1.0-1_all.deb
#    sudo apt-get update
#    sudo apt-get -y install cuda-toolkit-11-8
#    
#    # Add to PATH (add to ~/.bashrc)
#    export PATH=/usr/local/cuda/bin:$PATH
#    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
#
#    WSL2:
#    -----
#    # Same as Linux, but ensure you have:
#    # 1. Windows 11 or Windows 10 (21H2+)
#    # 2. NVIDIA GPU drivers installed on Windows (not in WSL)
#    # 3. WSL2 (not WSL1): wsl --set-version Ubuntu 2
#
#    MACOS:
#    ------
#    # CUDA not available on macOS
#    # Use MPS (Metal Performance Shaders) instead
#    # Set: DEVICE=mps
#
# 3. Install PyTorch with CUDA support:
#
#    # For CUDA 12.1
#    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
#    
#    # For CUDA 11.8
#    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
#
# 4. Verify installation:
#    python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
#    python -c "import torch; print(f'CUDA version: {torch.version.cuda}')"
#
# 5. Common issues:
#    - "CUDA not available": Check NVIDIA drivers (nvidia-smi)
#    - "CUDA version mismatch": Reinstall PyTorch matching your CUDA version
#    - "Out of memory": Reduce batch size or enable quantization
#
# ============================================================================
# HARDWARE-SPECIFIC RECOMMENDATIONS
# ============================================================================
#
# HIGH-END GPU (RTX 3090/4090, A100, V100, 16GB+ VRAM):
# ------------------------------------------------------
# DEVICE=cuda
# MIXED_PRECISION=fp16
# QUANTIZATION=none
# ENABLE_TORCH_COMPILE=auto
# Expected RTF: 0.8x (faster than real-time)
#
# MID-RANGE GPU (RTX 3060/4060, 8-16GB VRAM):
# --------------------------------------------
# DEVICE=cuda
# MIXED_PRECISION=fp16
# QUANTIZATION=int8
# ENABLE_TORCH_COMPILE=auto
# Expected RTF: 1.2x (slightly slower than real-time)
#
# LOW-END GPU (GTX 1660, 6GB VRAM):
# ----------------------------------
# DEVICE=cuda
# MIXED_PRECISION=fp16
# QUANTIZATION=int8
# MAX_TEXT_LENGTH=600
# Expected RTF: 2.0x (2x slower than real-time)
#
# APPLE SILICON (M1/M2/M3):
# -------------------------
# DEVICE=mps
# MIXED_PRECISION=fp16
# QUANTIZATION=int8
# ENABLE_TORCH_COMPILE=false  # Unstable on macOS
# Expected RTF: 2.0x
#
# CPU ONLY (No GPU):
# -------------------
# DEVICE=cpu
# MIXED_PRECISION=fp32
# QUANTIZATION=int8
# ENABLE_TORCH_COMPILE=false
# OMP_NUM_THREADS=8  # Set to your CPU core count
# Expected RTF: 10-20x (very slow)
#
# MULTI-GPU SETUP:
# ----------------
# Use CUDA_VISIBLE_DEVICES to select GPU:
# CUDA_VISIBLE_DEVICES=0  # Use first GPU
# CUDA_VISIBLE_DEVICES=1  # Use second GPU
# CUDA_VISIBLE_DEVICES=0,1  # Use both (not supported yet)
#
# ============================================================================


# EMERGENCY FIXES for macOS M1 Air Issues
# Force CPU mode to prevent MPS/gradient checkpointing conflicts
PYTORCH_ENABLE_MPS_FALLBACK=0
PYTORCH_MPS_NO_FORK=1
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4