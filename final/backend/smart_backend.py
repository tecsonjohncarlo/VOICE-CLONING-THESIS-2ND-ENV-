"""
Smart Adaptive Backend for Fish Speech
Automatically detects hardware and self-optimizes in real-time

Integration: Replace engine initialization in app.py with SmartAdaptiveBackend
"""
import os
import sys
import platform
import subprocess
import psutil
import torch
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger
import numpy as np

@dataclass
class HardwareProfile:
    """Complete hardware profile with all capabilities"""
    device_type: str  # 'cuda', 'mps', 'cpu'
    device_name: str
    cpu_model: str
    cpu_tier: str  # 'high_end', 'mid_range', 'low_end', 'm1_air', 'm1_pro'
    cores_physical: int
    cores_logical: int
    memory_gb: float
    has_gpu: bool
    gpu_memory_gb: float
    gpu_name: str
    system: str  # 'Windows', 'Darwin', 'Linux'
    machine: str  # 'x86_64', 'AMD64', 'arm64', etc.
    thermal_capable: bool
    avx512_vnni: bool  # For INT8 CPU optimization
    compute_capability: Optional[Tuple[int, int]]  # For CUDA GPUs

@dataclass
class OptimalConfig:
    """Auto-selected optimal configuration"""
    device: str
    precision: str  # 'fp32', 'fp16', 'bf16'
    quantization: str  # 'none', 'int8', 'int4'
    useonnx: bool
    usetorchcompile: bool
    chunk_length: int
    max_batch_size: int
    num_threads: int
    cache_limit: int
    enable_thermal_management: bool
    expected_rtf: float
    expected_memory_gb: float
    optimization_strategy: str
    notes: str
    use_gradient_checkpointing: bool = False # For training, not inference
    max_text_length: int = 500  # Maximum text length for this hardware


class SmartHardwareDetector:
    """Comprehensive hardware detection and profiling"""
    
    def __init__(self):
        self.system = platform.system()
        self.machine = platform.machine()
        self.is_wsl = self._detect_wsl()
        self.profile = self._build_complete_profile()
        logger.info("üîç Hardware detection complete")
        self._log_detection_results()
    
    def _detect_wsl(self) -> bool:
        """Detect if running in WSL2"""
        if self.system != 'Linux':
            return False
        
        try:
            # Check for WSL in kernel version
            with open('/proc/version', 'r') as f:
                version = f.read().lower()
                is_wsl = 'microsoft' in version or 'wsl' in version
                if is_wsl:
                    logger.info("‚úÖ Running in WSL2 - Triton support available!")
                return is_wsl
        except:
            return False
    
    def _build_complete_profile(self) -> HardwareProfile:
        """Build complete hardware profile"""
        # CPU detection
        cpu_info = self._get_cpu_info()
        cpu_tier = self._detect_cpu_tier(cpu_info)
        
        # GPU detection
        gpu_info = self._detect_gpu()
        
        # Capability detection
        thermal_capable = self._check_thermal_capability()
        avx512_vnni = self._check_avx512_vnni()
        
        return HardwareProfile(
            device_type=gpu_info['device_type'],
            device_name=gpu_info['device_name'],
            cpu_model=cpu_info['model'],
            cpu_tier=cpu_tier,
            cores_physical=cpu_info['cores_physical'],
            cores_logical=cpu_info['cores_logical'],
            memory_gb=psutil.virtual_memory().total / (1024**3),
            has_gpu=gpu_info['has_gpu'],
            gpu_memory_gb=gpu_info['gpu_memory_gb'],
            gpu_name=gpu_info['gpu_name'],
            system=self.system,
            machine=self.machine,
            thermal_capable=thermal_capable,
            avx512_vnni=avx512_vnni,
            compute_capability=gpu_info['compute_capability']
        )
    
    def _get_cpu_info(self) -> Dict[str, Any]:
        """Detailed CPU information"""
        return {
            'model': platform.processor(),
            'cores_physical': psutil.cpu_count(logical=False) or 1,
            'cores_logical': psutil.cpu_count(logical=True) or 1,
            'frequency_mhz': psutil.cpu_freq().max if psutil.cpu_freq() else 0
        }
    
    def _detect_cpu_tier(self, cpu_info: Dict) -> str:
        """Classify CPU performance tier"""
        model = cpu_info['model'].lower()
        cores = cpu_info['cores_physical']
        
        # Apple Silicon
        if self.system == 'Darwin' and 'arm' in self.machine.lower():
            return 'm1_pro' if self._is_m1_pro_or_better() else 'm1_air'
        
        # Intel
        if 'intel' in model:
            if any(x in model for x in ['i9', 'i7', 'ultra']) and cores >= 8:
                return 'intel_high_end'
            elif 'i5' in model and any(x in model for x in ['12', '13', '14']):
                return 'i5_baseline'
            else:
                return 'intel_low_end'
        
        # AMD
        if 'amd' in model or 'ryzen' in model:
            if any(x in model for x in ['ryzen 9', 'ryzen 7', 'threadripper']) and cores >= 8:
                return 'amd_high_end'
            else:
                return 'amd_mobile'
        
        # ARM SBC
        if 'arm' in self.machine.lower() and cores <= 4:
            return 'arm_sbc'
        
        return 'intel_low_end'  # Conservative default
    
    def _is_m1_pro_or_better(self) -> bool:
        """Detect M1 Pro/Max/Ultra vs M1 Air"""
        try:
            result = subprocess.run(
                ['system_profiler', 'SPHardwareDataType'],
                capture_output=True, text=True, timeout=5
            )
            return any(x in result.stdout for x in ['MacBook Pro', 'Mac Studio', 'iMac', 'Mac Mini'])
        except:
            return False
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """Comprehensive GPU detection"""
        gpu_info = {
            'has_gpu': False,
            'device_type': 'cpu',
            'device_name': 'CPU',
            'gpu_name': 'None',
            'gpu_memory_gb': 0.0,
            'compute_capability': None
        }
        
        # CUDA detection
        if torch.cuda.is_available():
            try:
                gpu_info.update({
                    'has_gpu': True,
                    'device_type': 'cuda',
                    'device_name': torch.cuda.get_device_name(0),
                    'gpu_name': torch.cuda.get_device_name(0),
                    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3),
                    'compute_capability': torch.cuda.get_device_capability(0)
                })
                logger.info(f"‚úÖ NVIDIA GPU detected: {gpu_info['gpu_name']}")
            except Exception as e:
                logger.warning(f"CUDA detected but error: {e}")
        
        # Apple Silicon MPS
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            gpu_info.update({
                'has_gpu': True,
                'device_type': 'mps',
                'device_name': 'Apple Silicon GPU',
                'gpu_name': 'Apple Silicon GPU',
                'gpu_memory_gb': psutil.virtual_memory().total / (1024**3) * 0.6  # Estimate
            })
            logger.info("‚úÖ Apple Silicon GPU (MPS) detected")
        
        return gpu_info
    
    def _check_thermal_capability(self) -> bool:
        """Check if thermal monitoring is available"""
        if self.system == 'Linux':
            return Path('/sys/class/thermal').exists()
        elif self.system == 'Darwin':
            try:
                subprocess.run(['powermetrics', '--version'], capture_output=True, timeout=2)
                return True
            except:
                return False
        elif self.system == 'Windows':
            # Check for common tools
            try:
                import wmi
                w = wmi.WMI(namespace="root\\LibreHardwareMonitor")
                return len(w.Sensor()) > 0
            except:
                return False
        return False
    
    def _check_avx512_vnni(self) -> bool:
        """Check for AVX-512 VNNI support (INT8 optimization)"""
        if self.system == 'Linux':
            try:
                with open('/proc/cpuinfo', 'r') as f:
                    cpuinfo = f.read()
                    return 'avx512_vnni' in cpuinfo
            except:
                pass
        # For Windows/macOS, check CPU model
        cpu_model = platform.processor().lower()
        # Intel 12th gen and newer have VNNI
        return any(x in cpu_model for x in ['i5-12', 'i5-13', 'i5-14', 'i7-12', 'i7-13', 'i7-14', 'i9-12', 'i9-13', 'i9-14'])
    
    def _log_detection_results(self):
        """Log detected hardware"""
        p = self.profile
        logger.info("="*70)
        logger.info("DETECTED HARDWARE PROFILE")
        logger.info("="*70)
        logger.info(f"System: {p.system} ({p.machine})")
        logger.info(f"CPU: {p.cpu_model}")
        logger.info(f"CPU Tier: {p.cpu_tier}")
        logger.info(f"Cores: {p.cores_physical} physical, {p.cores_logical} logical")
        logger.info(f"RAM: {p.memory_gb:.1f} GB")
        logger.info(f"")
        logger.info(f"GPU: {p.gpu_name}")
        logger.info(f"GPU Memory: {p.gpu_memory_gb:.1f} GB")
        logger.info(f"Device: {p.device_type}")
        logger.info(f"")
        logger.info(f"Thermal Monitoring: {'‚úÖ Available' if p.thermal_capable else '‚ùå Not Available'}")
        logger.info(f"AVX-512 VNNI: {'‚úÖ Supported' if p.avx512_vnni else '‚ùå Not Supported'}")
        if p.compute_capability:
            logger.info(f"CUDA Compute: {p.compute_capability[0]}.{p.compute_capability[1]}")
        logger.info("="*70)


class ConfigurationSelector:
    """Select optimal configuration based on hardware profile"""
    
    def __init__(self, profile: HardwareProfile, is_wsl: bool = False):
        self.profile = profile
        self.is_wsl = is_wsl
    
    def select_optimal_config(self) -> OptimalConfig:
        """Select best configuration for detected hardware"""
        
        # GPU-based configuration
        # CRITICAL FIX: Increased threshold from 3.5GB to 6GB
        # RTX 3050 4GB causes memory overflow (uses 5.97GB, swaps to RAM)
        # 6GB+ GPUs (RTX 3060, RTX 4060, etc.) work reliably
        # 4GB GPUs should use CPU mode with ONNX for better performance
        logger.debug(f"GPU check: has_gpu={self.profile.has_gpu}, gpu_memory_gb={self.profile.gpu_memory_gb:.2f}")
        if self.profile.has_gpu and self.profile.gpu_memory_gb >= 6.0:
            logger.info(f"‚úÖ GPU configuration selected (GPU: {self.profile.gpu_name}, {self.profile.gpu_memory_gb:.2f}GB VRAM)")
            return self._gpu_config()
        
        # Log why GPU wasn't selected
        if self.profile.has_gpu:
            logger.warning(f"‚ö†Ô∏è GPU detected but insufficient VRAM: {self.profile.gpu_memory_gb:.2f}GB < 6.0GB required")
            logger.warning(f"‚ö†Ô∏è 4GB GPUs cause memory overflow (5.97GB usage) - using CPU mode instead")
            logger.info(f"üí° CPU mode with ONNX will provide better performance than overloaded GPU")
        else:
            logger.info("‚ÑπÔ∏è No GPU detected, using CPU configuration")
        
        # CPU-based configuration by tier
        tier_configs = {
            'intel_high_end': self._high_end_cpu_config,
            'amd_high_end': self._high_end_cpu_config,
            'm1_pro': self._m1_pro_config,
            'm1_air': self._m1_air_config,
            'i5_baseline': self._i5_baseline_config,
            'intel_low_end': self._low_end_cpu_config,
            'amd_mobile': self._mobile_cpu_config,
            'arm_sbc': self._arm_sbc_config
        }
        
        config_fn = tier_configs.get(self.profile.cpu_tier, self._low_end_cpu_config)
        return config_fn()
    
    def _gpu_config(self) -> OptimalConfig:
        """Configuration for GPU inference"""
        # CRITICAL FIX: 4GB GPUs need extreme memory optimization
        # RTX 3050 Laptop (4GB) was using 5.97GB causing memory overflow to RAM
        if self.profile.gpu_memory_gb <= 4.5:
            logger.warning(f"‚ö†Ô∏è 4GB GPU detected ({self.profile.gpu_name}) - applying extreme memory optimization")
            return OptimalConfig(
                device='cuda',
                precision='fp16',  # Force fp16, not bf16
                quantization='none',  # Disable INT8 - not reducing memory effectively
                useonnx=False,
                usetorchcompile=False,  # Disabled on Windows
                chunk_length=200,  # CRITICAL: Reduced from 1024 to 200
                max_batch_size=1,  # Force batch size 1
                num_threads=4,
                cache_limit=25,  # Reduced from 100
                enable_thermal_management=self.profile.thermal_capable,
                expected_rtf=5.0,  # More realistic for 4GB GPU
                expected_memory_gb=3.5,  # Stay under 4GB
                optimization_strategy='extreme_4gb_optimization',
                notes='‚ö†Ô∏è Extreme memory optimization for 4GB VRAM - chunk_length=200, no quantization, fp16 only',
                max_text_length=200  # CRITICAL: Reduced from 600 to 200
            )
        
        # Determine precision based on compute capability
        precision = 'fp16'
        if self.profile.device_type == 'cuda' and self.profile.compute_capability:
            if self.profile.compute_capability[0] >= 8:  # Ampere+
                precision = 'bf16'
        
        # torch.compile support:
        # 1. WSL2 (Linux with Microsoft kernel) - ‚úÖ Has Triton (best performance)
        # 2. Native Linux (NVIDIA GPU) - ‚úÖ Has Triton (best performance)
        # 3. macOS (MPS) - ‚ùå UNSTABLE - causes hanging/freezing
        # 4. Windows - ‚ùå Triton not available, inductor unstable, disabled by default
        # 5. User override with FORCE_TORCH_COMPILE=1
        
        force_compile = os.getenv('FORCE_TORCH_COMPILE', '0') == '1'
        
        # Smart torch.compile enablement
        if force_compile:
            use_compile = True
            logger.info("üîß torch.compile FORCED via FORCE_TORCH_COMPILE=1")
        elif self.is_wsl and self.profile.has_gpu:
            use_compile = True
            logger.info("üöÄ WSL2 + NVIDIA GPU detected - enabling torch.compile with Triton (20-30% speedup!)")
        elif self.profile.system == 'Linux' and self.profile.device_type == 'cuda':
            # Native Linux with NVIDIA GPU
            use_compile = True
            logger.info("üöÄ Linux + NVIDIA GPU detected - enabling torch.compile with Triton")
        elif self.profile.system == 'Darwin':
            # macOS: torch.compile with MPS is UNSTABLE (causes hanging)
            use_compile = False
            logger.warning("‚ö†Ô∏è macOS detected: torch.compile disabled (MPS backend unstable, causes hanging)")
        elif self.profile.system == 'Windows':
            # Windows: Disabled (Triton not available, inductor unstable)
            use_compile = False
            logger.info("‚ö†Ô∏è Windows detected: torch.compile disabled (Triton not available, use WSL2 for 20-30% speedup)")
        else:
            # Unknown platform: disable for safety
            use_compile = False
            logger.warning(f"‚ö†Ô∏è Unknown platform ({self.profile.system}): torch.compile disabled for safety")
        
        # Smart quantization based on GPU tier
        if self.profile.gpu_memory_gb >= 12:
            # High-end GPU: No quantization needed (V100, A100, RTX 3090, 4090)
            quantization = 'none'
            expected_memory = 6.0  # Full precision model
            expected_rtf = 0.8  # Faster than real-time
        elif self.profile.gpu_memory_gb >= 8:
            # Mid-range GPU: Light quantization (RTX 3060, 4060)
            quantization = 'int8'
            expected_memory = 4.0
            expected_rtf = 1.2
        else:
            # Entry-level GPU (6GB): Moderate quantization
            quantization = 'int8'
            expected_memory = 4.5
            expected_rtf = 2.0
        
        # Determine max text length based on GPU tier
        if self.profile.gpu_memory_gb >= 16:
            max_text = 2000  # High-end GPU (RTX 3090, 4090, A100, V100)
        elif self.profile.gpu_memory_gb >= 8:
            max_text = 1000  # Mid-range GPU (RTX 3060, 4060)
        else:
            max_text = 600   # Entry-level GPU (6GB+)
        
        return OptimalConfig(
            device='cuda' if self.profile.device_type == 'cuda' else 'mps',
            precision=precision,
            quantization=quantization,
            useonnx=False,  # Not needed for GPU
            usetorchcompile=use_compile,
            chunk_length=1024,  # Large chunks for GPU
            max_batch_size=4,
            num_threads=self.profile.cores_physical // 2,
            cache_limit=100,
            enable_thermal_management=self.profile.thermal_capable,
            expected_rtf=expected_rtf,
            expected_memory_gb=expected_memory,
            optimization_strategy='gpu_optimized',
            notes=f'GPU-accelerated with {precision.upper()} precision' + (f', {quantization.upper()} quantization' if quantization != 'none' else ', no quantization') + (
                ' + torch.compile (Triton)' if use_compile else
                ' (torch.compile disabled - use WSL2 for 20-30% speedup)' if self.profile.system == 'Windows' else
                ' (torch.compile disabled)'
            ),
            max_text_length=max_text
        )
    
    def _m1_pro_config(self) -> OptimalConfig:
        """M1 Pro/Max/Ultra configuration"""
        # CRITICAL FIX: Respect user device preference
        device = 'cpu' if not self.profile.has_gpu else 'mps'
        
        return OptimalConfig(
            device=device,
            precision='fp16',
            quantization='int8',
            useonnx=False,
            usetorchcompile=False,  # Works well on M1
            chunk_length=512,
            max_batch_size=2,
            num_threads=4,
            cache_limit=50,
            enable_thermal_management=True,
            expected_rtf=12.0,
            expected_memory_gb=3.0,
            optimization_strategy='m1_pro_sustained',
            notes='Active cooling maintains sustained performance',
            max_text_length=400  # M1 Pro/Max with active cooling
        )
    
    def _m1_air_config(self) -> OptimalConfig:
        """M1 Air configuration with thermal management"""
        # Respect user's torch.compile preference (default: False for stability)
        user_torch_compile = os.getenv('ENABLE_TORCH_COMPILE', 'false').lower() == 'true'
        
        # CRITICAL FIX: Respect user device preference
        device = 'cpu' if not self.profile.has_gpu else 'mps'
        
        # ============================================
        # CRITICAL: Set macOS-specific environment variables for MPS stability
        # ============================================
        if device == 'mps':
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"  # Allow CPU fallback for unsupported ops
            os.environ["OMP_NUM_THREADS"] = "4"
            os.environ["MKL_NUM_THREADS"] = "4"
            logger.info("‚úÖ M1 Air: Environment variables configured for MPS stability")
            logger.info("   PYTORCH_ENABLE_MPS_FALLBACK=1 (CPU fallback enabled)")
            logger.info("   OMP_NUM_THREADS=4, MKL_NUM_THREADS=4")
        
        return OptimalConfig(
            device=device,
            precision='fp16',
            quantization='none',  # Disabled: INT8 on MPS causes CPU fallback
            useonnx=False,
            usetorchcompile=user_torch_compile,  # Respect user preference
            chunk_length=1024,  # Larger chunks for better performance
            max_batch_size=1,
            num_threads=4,
            cache_limit=25,
            enable_thermal_management=True,
            expected_rtf=2.4, # Projected RTF after fix
            expected_memory_gb=2.5,
            optimization_strategy='m1_air_fp16_optimized',
            notes=f'FP16 only for MPS backend; INT8 disabled. torch.compile: {"enabled" if user_torch_compile else "disabled (macOS limitation)"}.',
            use_gradient_checkpointing=False, # Disabled for inference
            max_text_length=600  # M1 Air - conservative due to thermal throttling
        )
    
    def _high_end_cpu_config(self) -> OptimalConfig:
        """High-end desktop CPU configuration"""
        return OptimalConfig(
            device='cpu',
            precision='fp32',
            quantization='int8',
            useonnx=False, 
            usetorchcompile=True,
            chunk_length=512,
            max_batch_size=2,
            num_threads=self.profile.cores_physical,
            cache_limit=50,
            enable_thermal_management=self.profile.thermal_capable,
            expected_rtf=3.0,
            expected_memory_gb=4.0,
            optimization_strategy='cpu_onnx_optimized',
            notes='High-end CPU with ONNX Runtime (4-5x faster)',
            max_text_length=500  # Intel i7/i9, AMD Ryzen 7/9
        )
    
    def _i5_baseline_config(self) -> OptimalConfig:
        """Intel i5 baseline configuration (YOUR LAPTOP)"""
        return OptimalConfig(
            device='cpu',
            precision='fp32',
            quantization='int8',
            useonnx=False,  # ‚úÖ Critical for i5
            usetorchcompile=False,  # Not helpful for single-use
            chunk_length=512,
            max_batch_size=1,
            num_threads=10,  # i5-1334U/1235U has 10 cores
            cache_limit=25,
            enable_thermal_management=True,
            expected_rtf=8.0,  # With ONNX optimization
            expected_memory_gb=3.5,
            optimization_strategy='i5_onnx_thermal',
            notes='ONNX Runtime + INT8 quantization (6x faster than baseline)',
            max_text_length=300  # Intel i5 baseline
        )
    
    def _low_end_cpu_config(self) -> OptimalConfig:
        """Low-end CPU configuration"""
        return OptimalConfig(
            device='cpu',
            precision='fp32',
            quantization='int8',
            useonnx=False,
            usetorchcompile=False,
            chunk_length=256,
            max_batch_size=1,
            num_threads=self.profile.cores_physical,
            cache_limit=10,
            enable_thermal_management=True,
            expected_rtf=12.0,
            expected_memory_gb=3.0,
            optimization_strategy='conservative_cpu',
            notes='Conservative settings for stability',
            max_text_length=200  # Low-end Intel/AMD
        )
    
    def _mobile_cpu_config(self) -> OptimalConfig:
        """Mobile CPU configuration"""
        return OptimalConfig(
            device='cpu',
            precision='fp32',
            quantization='int8',
            useonnx=True,
            usetorchcompile=False,
            chunk_length=256,
            max_batch_size=1,
            num_threads=min(4, self.profile.cores_physical),
            cache_limit=15,
            enable_thermal_management=True,
            expected_rtf=10.0,
            expected_memory_gb=3.0,
            optimization_strategy='mobile_efficient',
            notes='Mobile-optimized with aggressive thermal management',
            max_text_length=250  # Mobile AMD/Intel
        )
    
    def _arm_sbc_config(self) -> OptimalConfig:
        """ARM SBC (Raspberry Pi, etc.) configuration"""
        return OptimalConfig(
            device='cpu',
            precision='fp32',
            quantization='int4',  # Aggressive
            useonnx=True,
            usetorchcompile=False,
            chunk_length=128,
            max_batch_size=1,
            num_threads=self.profile.cores_physical,
            cache_limit=5,
            enable_thermal_management=True,
            expected_rtf=25.0,
            expected_memory_gb=1.5,
            optimization_strategy='extreme_memory_saver',
            notes='‚ö†Ô∏è Proof of concept only - very slow',
            max_text_length=100  # Raspberry Pi - very conservative
        )


class MemoryBudgetManager:
    """Strictly enforce memory budgets to prevent throttling"""
    
    def __init__(self, hardware_profile: HardwareProfile):
        self.profile = hardware_profile
        self.memory_budget = self._calculate_memory_budget()
        self.current_usage = 0
        
    def _calculate_memory_budget(self) -> dict:
        """Calculate safe memory budgets per hardware tier"""
        budgets = {
            "m1_air": {
                "total_gb": 8,
                "reserved_for_os": 2.0,
                "max_model": 2.0,
                "max_cache": 1.0,
                "max_batch": 0.5,
                "safety_margin": 1.5,
            },
            "m1_pro": {
                "total_gb": 16,
                "reserved_for_os": 2.5,
                "max_model": 4.0,
                "max_cache": 2.0,
                "max_batch": 1.5,
                "safety_margin": 2.0,
            },
            "m1_max": {
                "total_gb": 32,
                "reserved_for_os": 3.0,
                "max_model": 8.0,
                "max_cache": 4.0,
                "max_batch": 2.0,
                "safety_margin": 3.0,
            },
            "intel_i5": {
                "total_gb": 16,
                "reserved_for_os": 3.0,
                "max_model": 3.0,
                "max_cache": 1.5,
                "max_batch": 0.8,
                "safety_margin": 2.0,
            },
            "intel_high_end": {
                "total_gb": 32,
                "reserved_for_os": 4.0,
                "max_model": 6.0,
                "max_cache": 3.0,
                "max_batch": 2.0,
                "safety_margin": 3.0,
            },
            "amd_ryzen5": {
                "total_gb": 16,
                "reserved_for_os": 3.0,
                "max_model": 3.0,
                "max_cache": 1.5,
                "max_batch": 0.8,
                "safety_margin": 2.0,
            },
            "amd_high_end": {
                "total_gb": 32,
                "reserved_for_os": 4.0,
                "max_model": 6.0,
                "max_cache": 3.0,
                "max_batch": 2.0,
                "safety_margin": 3.0,
            },
        }
        
        tier = self.profile.cpu_tier
        if tier not in budgets:
            return budgets.get("intel_i5")  # Conservative default
        
        return budgets[tier]
    
    def enforce_limits(self) -> bool:
        """Check if memory usage exceeds safe limits"""
        current_memory_gb = psutil.virtual_memory().used / (1024**3)
        available_gb = psutil.virtual_memory().available / (1024**3)
        
        budget = self.memory_budget
        safe_limit = (budget["total_gb"] - 
                     budget["reserved_for_os"] - 
                     budget["safety_margin"])
        
        if available_gb < budget["safety_margin"]:
            logger.error(f"‚ö†Ô∏è CRITICAL: Only {available_gb:.1f}GB available (need {budget['safety_margin']}GB safety margin)")
            return False
        
        return True
    
    def get_adjusted_cache_limit(self) -> int:
        """Get cache limit that respects memory budget"""
        available_gb = psutil.virtual_memory().available / (1024**3)
        max_cache = self.memory_budget["max_cache"]
        
        # Conservative: use only 50% of budgeted cache
        suggested_cache_mb = int((min(available_gb, max_cache) * 0.5) * 1024)
        
        logger.info(f"Cache limit: {suggested_cache_mb}MB (available: {available_gb:.1f}GB)")
        return suggested_cache_mb


class QuantizationStrategy:
    """Advanced quantization for low-memory devices"""
    
    @staticmethod
    def get_quantization_config(cpu_tier: str, available_memory_gb: float) -> dict:
        """Return quantization strategy per device and memory"""
        
        strategies = {
            "m1_air": {
                "available_memory_gb": 8,
                "quantization": "int8",
                "layer_wise_quant": True,  # Quantize layer-by-layer
                "weight_only": True,  # Only quantize weights, not activations
                "dynamic_quant": True,  # Per-tensor quantization
                "calibration_data_size": 32,  # Small calibration dataset
            },
            "m1_pro": {
                "available_memory_gb": 16,
                "quantization": "int8",
                "layer_wise_quant": True,
                "weight_only": True,
                "dynamic_quant": True,
                "calibration_data_size": 64,
            },
            "intel_i5": {
                "available_memory_gb": 16,
                "quantization": "int8",
                "layer_wise_quant": True,
                "weight_only": True,
                "dynamic_quant": True,
                "calibration_data_size": 64,
            },
            "amd_ryzen5": {
                "available_memory_gb": 16,
                "quantization": "int8",
                "layer_wise_quant": True,
                "weight_only": True,
                "dynamic_quant": True,
                "calibration_data_size": 64,
            },
            "intel_high_end": {
                "available_memory_gb": 32,
                "quantization": "int8",
                "layer_wise_quant": False,  # Can handle full quantization
                "weight_only": False,
                "dynamic_quant": True,
                "calibration_data_size": 128,
            },
        }
        
        config = strategies.get(cpu_tier, strategies["intel_i5"])
        
        # Adaptive: if memory is really low, use int4
        if available_memory_gb < 6:
            config["quantization"] = "int4"
            logger.warning(f"‚ö†Ô∏è Aggressive quantization: INT4 (low memory: {available_memory_gb:.1f}GB)")
        
        return config


class CPUAffinityManager:
    """Optimize thread affinity to reduce context switching on low-end CPUs"""
    
    def __init__(self, profile: HardwareProfile):
        self.profile = profile
        
    def set_thread_affinity(self):
        """Set optimal thread affinity based on CPU tier"""
        try:
            import psutil
            
            if self.profile.cpu_tier == "intel_i5":
                # i5 typically: 2P cores (performance) + 8E cores (efficiency)
                # Pin inference to P-cores only for better latency
                p_cores = list(range(0, min(2, self.profile.cores_physical)))
                psutil.Process().cpu_affinity(p_cores)
                logger.info(f"‚úÖ Pinned to P-cores: {p_cores}")
                
            elif self.profile.cpu_tier in ["amd_ryzen5", "amd_high_end"]:
                # AMD: typically 6+ cores uniform
                # Use even core indices for better cache locality
                cores = list(range(0, self.profile.cores_physical, 2))
                psutil.Process().cpu_affinity(cores)
                logger.info(f"‚úÖ Pinned to cores: {cores}")
                
            elif self.profile.cpu_tier == "m1_air":
                # M1 Air: 4 performance + 4 efficiency cores
                # Pin to performance cores only
                perf_cores = list(range(0, 4))
                logger.info(f"‚úÖ M1 Air: Using performance cores {perf_cores}")
                # Note: macOS doesn't support cpu_affinity, but we log the intent
                
        except Exception as e:
            logger.warning(f"CPU affinity not available: {e}")


class ResourceMonitor:
    """Real-time resource monitoring and adaptive adjustment"""
    
    def __init__(self, profile: HardwareProfile):
        self.profile = profile
        self.gpu_util_history = []
        self.memory_history = []
        self.temp_history = []
        self.memory_budget_manager = MemoryBudgetManager(profile)
        self.cpu_affinity_manager = CPUAffinityManager(profile)
    
    def check_resources(self) -> Dict[str, Any]:
        """Check current resource usage"""
        resources = {
            'cpu_percent': psutil.cpu_percent(interval=0.1),
            'memory_percent': psutil.virtual_memory().percent,
            'memory_available_gb': psutil.virtual_memory().available / (1024**3)
        }
        
        # GPU monitoring
        if self.profile.device_type == 'cuda':
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                resources['gpu_util'] = util.gpu
                resources['gpu_memory_used_gb'] = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024**3)
                pynvml.nvmlShutdown()
            except:
                resources['gpu_util'] = 0
                resources['gpu_memory_used_gb'] = 0
        
        return resources
    
    def should_throttle(self, resources: Dict) -> bool:
        """Determine if throttling needed"""
        if resources['memory_percent'] > 90:
            logger.warning(f"‚ö†Ô∏è Memory usage critical: {resources['memory_percent']:.1f}%")
            return True
        
        if resources['cpu_percent'] > 95:
            logger.warning(f"‚ö†Ô∏è CPU usage critical: {resources['cpu_percent']:.1f}%")
            return True
        
        return False
    
    def suggest_adjustment(self, resources: Dict, current_config: OptimalConfig) -> Optional[OptimalConfig]:
        """Suggest configuration adjustment based on resources"""
        if self.should_throttle(resources):
            logger.info("üîß Auto-adjusting configuration to reduce resource usage")
            # Create adjusted config
            adjusted = OptimalConfig(
                device=current_config.device,
                precision=current_config.precision,
                quantization=current_config.quantization,
                useonnx=current_config.useonnx,
                usetorchcompile=current_config.usetorchcompile,
                chunk_length=current_config.chunk_length // 2,  # Smaller chunks
                max_batch_size=1,  # Force batch size 1
                num_threads=max(1, current_config.num_threads // 2),  # Reduce threads
                cache_limit=current_config.cache_limit // 2,  # Reduce cache
                enable_thermal_management=True,
                expected_rtf=current_config.expected_rtf * 1.3,
                expected_memory_gb=current_config.expected_memory_gb * 0.7,
                optimization_strategy=f"{current_config.optimization_strategy}_throttled",
                notes=f"{current_config.notes} (auto-throttled due to resource pressure)"
            )
            return adjusted
        return None


class IntelligentDeviceSelector:
    """
    Intelligent device selector that monitors system load and chooses optimal device
    
    Features:
    - Real-time GPU/CPU utilization monitoring
    - Memory pressure detection
    - Thermal monitoring (if available)
    - Automatic device switching when DEVICE=auto
    - Respects user preference when DEVICE is explicitly set
    """
    
    def __init__(self, profile: HardwareProfile, user_preference: str = 'auto', device_locked: bool = False):
        self.profile = profile
        self.user_preference = user_preference
        self.device_locked = device_locked
        self.last_check_time = 0
        self.check_interval = 5.0  # Check every 5 seconds
        
        # Thresholds for device switching
        self.gpu_util_threshold = 85.0  # Switch to CPU if GPU >85% utilized
        self.gpu_memory_threshold = 90.0  # Switch to CPU if GPU memory >90%
        self.cpu_util_threshold = 90.0  # Switch to GPU if CPU >90% utilized
        self.system_memory_threshold = 85.0  # Switch based on RAM pressure
        
        logger.info(f"üß† Intelligent Device Selector initialized")
        logger.info(f"   User preference: {user_preference}")
        logger.info(f"   Device locked: {device_locked}")
    
    def get_optimal_device(self, current_device: str = None) -> dict:
        """
        Determine optimal device based on current system state
        
        Returns:
            dict with 'device', 'reason', and 'should_switch' keys
        """
        import time
        
        # If device is locked by user, always return user preference
        if self.device_locked:
            return {
                'device': self.user_preference,
                'reason': 'User preference (locked)',
                'should_switch': False
            }
        
        # Rate limit checks to avoid overhead
        current_time = time.time()
        if current_time - self.last_check_time < self.check_interval:
            return {
                'device': current_device or self.profile.device_type,
                'reason': 'Using current device (check interval not elapsed)',
                'should_switch': False
            }
        
        self.last_check_time = current_time
        
        # Get current system state
        system_state = self._get_system_state()
        
        # Decision logic
        decision = self._make_device_decision(system_state, current_device)
        
        return decision
    
    def _get_system_state(self) -> dict:
        """Get current system resource utilization"""
        state = {
            'cpu_percent': psutil.cpu_percent(interval=0.1),
            'memory_percent': psutil.virtual_memory().percent,
            'gpu_available': self.profile.has_gpu,
            'gpu_util': 0.0,
            'gpu_memory_percent': 0.0,
            'thermal_throttling': False
        }
        
        # Get GPU stats if available
        if self.profile.device_type == 'cuda' and torch.cuda.is_available():
            try:
                # GPU utilization (approximate via memory usage)
                gpu_mem_allocated = torch.cuda.memory_allocated(0) / torch.cuda.get_device_properties(0).total_memory * 100
                gpu_mem_reserved = torch.cuda.memory_reserved(0) / torch.cuda.get_device_properties(0).total_memory * 100
                state['gpu_memory_percent'] = max(gpu_mem_allocated, gpu_mem_reserved)
                
                # Try to get GPU utilization via nvidia-smi (if available)
                try:
                    import subprocess
                    result = subprocess.run(
                        ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
                        capture_output=True, text=True, timeout=1
                    )
                    if result.returncode == 0:
                        state['gpu_util'] = float(result.stdout.strip())
                except:
                    # Fallback: estimate from memory usage
                    state['gpu_util'] = state['gpu_memory_percent']
            except Exception as e:
                logger.debug(f"Could not get GPU stats: {e}")
        
        return state
    
    def _make_device_decision(self, state: dict, current_device: str) -> dict:
        """
        Make intelligent device selection based on system state
        
        Decision tree:
        1. If GPU overloaded (>85% util or >90% memory) ‚Üí Switch to CPU
        2. If CPU overloaded (>90% util) and GPU available ‚Üí Switch to GPU
        3. If system memory critical (>85%) ‚Üí Use device with less memory pressure
        4. Otherwise ‚Üí Keep current device (avoid thrashing)
        """
        current_device = current_device or self.profile.device_type
        
        # Check if GPU is overloaded
        if current_device == 'cuda' and state['gpu_available']:
            if state['gpu_util'] > self.gpu_util_threshold:
                return {
                    'device': 'cpu',
                    'reason': f"GPU overloaded ({state['gpu_util']:.1f}% utilization)",
                    'should_switch': True
                }
            if state['gpu_memory_percent'] > self.gpu_memory_threshold:
                return {
                    'device': 'cpu',
                    'reason': f"GPU memory critical ({state['gpu_memory_percent']:.1f}% used)",
                    'should_switch': True
                }
        
        # Check if CPU is overloaded and GPU is available
        if current_device == 'cpu' and state['gpu_available']:
            if state['cpu_percent'] > self.cpu_util_threshold:
                # Only switch to GPU if it's not also overloaded
                if state['gpu_util'] < 70.0 and state['gpu_memory_percent'] < 70.0:
                    return {
                        'device': 'cuda' if self.profile.device_type == 'cuda' else 'mps',
                        'reason': f"CPU overloaded ({state['cpu_percent']:.1f}%), GPU available",
                        'should_switch': True
                    }
        
        # Check system memory pressure
        if state['memory_percent'] > self.system_memory_threshold:
            # Prefer GPU if available (uses VRAM instead of system RAM)
            if state['gpu_available'] and current_device == 'cpu':
                if state['gpu_memory_percent'] < 70.0:
                    return {
                        'device': 'cuda' if self.profile.device_type == 'cuda' else 'mps',
                        'reason': f"System RAM critical ({state['memory_percent']:.1f}%), switching to GPU",
                        'should_switch': True
                    }
        
        # No issues detected, keep current device
        return {
            'device': current_device,
            'reason': f"System healthy (CPU: {state['cpu_percent']:.1f}%, RAM: {state['memory_percent']:.1f}%)",
            'should_switch': False
        }


class SmartAdaptiveBackend:
    """
    Intelligent adaptive backend that auto-detects and self-optimizes
    
    Features:
    - Auto-detects hardware capabilities
    - Monitors real-time system load
    - Dynamically switches devices when DEVICE=auto
    - Respects user preference when DEVICE is explicitly set
    
    Usage in app.py:
        from smart_backend import SmartAdaptiveBackend
        engine = SmartAdaptiveBackend(model_path="checkpoints/openaudio-s1-mini")
    """
    
    def __init__(self, model_path: str = "checkpoints/openaudio-s1-mini"):
        logger.info("üöÄ Initializing Smart Adaptive Backend")
        
        # Step 1: Detect hardware
        self.detector = SmartHardwareDetector()
        self.profile = self.detector.profile
        
        # Step 2: Check for user device preference
        self.user_device_preference = os.getenv('DEVICE', 'auto').lower()
        self.device_locked = False  # Track if user has locked device choice
        
        if self.user_device_preference != 'auto':
            logger.info(f"üë§ User device preference: {self.user_device_preference.upper()}")
            self._apply_user_device_preference(self.user_device_preference)
            self.device_locked = True  # User explicitly chose device, don't auto-switch
            logger.info("üîí Device locked to user preference (won't auto-switch)")
        else:
            logger.info("ü§ñ Smart auto-selection enabled (will optimize based on system load)")
            self.device_locked = False
        
        # Step 3: Select optimal configuration
        self.selector = ConfigurationSelector(self.profile, self.detector.is_wsl)
        self.config = self.selector.select_optimal_config()
        self._log_selected_config()
        
        # Step 3: Initialize resource monitor with memory budget manager
        self.monitor = ResourceMonitor(self.profile)
        
        # Step 4: Set CPU affinity for optimal performance
        self.monitor.cpu_affinity_manager.set_thread_affinity()
        
        # Step 5: Enforce memory limits before initialization
        if not self.monitor.memory_budget_manager.enforce_limits():
            logger.warning("‚ö†Ô∏è Memory constraints detected - using conservative settings")
        
        # Step 6: Apply configuration
        self._apply_configuration()
        
        # Step 7: Initialize engine with optimal settings
        self.engine = self._initialize_engine(model_path)
        
        # Step 8: Initialize intelligent device selector for runtime optimization
        self.intelligent_selector = IntelligentDeviceSelector(
            profile=self.profile,
            user_preference=self.user_device_preference,
            device_locked=self.device_locked
        )
        
        logger.info("‚úÖ Smart Adaptive Backend initialized successfully!")
        if not self.device_locked:
            logger.info("üí° Tip: Backend will auto-switch devices if system is overloaded")
            logger.info("üí° To lock device, set DEVICE=cpu or DEVICE=cuda in .env")
    
    def _log_selected_config(self):
        """Log selected configuration"""
        c = self.config
        logger.info("="*70)
        logger.info("SELECTED OPTIMAL CONFIGURATION")
        logger.info("="*70)
        logger.info(f"Strategy: {c.optimization_strategy}")
        logger.info(f"Device: {c.device}")
        logger.info(f"Precision: {c.precision}")
        logger.info(f"Quantization: {c.quantization}")
        logger.info(f"ONNX Runtime: {'‚úÖ Enabled' if c.useonnx else '‚ùå Disabled'}")
        logger.info(f"torch.compile: {'‚úÖ Enabled' if c.usetorchcompile else '‚ùå Disabled'}")
        logger.info(f"Chunk Length: {c.chunk_length}")
        logger.info(f"Threads: {c.num_threads}")
        logger.info(f"Max Text Length: {c.max_text_length} characters")
        logger.info(f"")
        logger.info(f"Expected Performance:")
        logger.info(f"  RTF: {c.expected_rtf:.1f}x (slower than real-time)")
        logger.info(f"  Memory: {c.expected_memory_gb:.1f} GB")
        logger.info(f"")
        logger.info(f"Notes: {c.notes}")
        logger.info("="*70)
    
    def _apply_user_device_preference(self, user_device: str):
        """Override auto-detected device with user preference"""
        if user_device == 'cpu':
            logger.info("‚úÖ Forcing CPU mode (user preference)")
            self.profile.device_type = 'cpu'
            self.profile.has_gpu = False
            
            # CRITICAL FIX: Disable MPS explicitly when forcing CPU (macOS)
            import os
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "0"
                logger.info("üîí MPS backend disabled - using CPU only")
            
            # CRITICAL FIX: Disable CUDA explicitly when forcing CPU (Windows/Linux)
            # Without this, downstream code sees CUDA available and overrides to GPU
            if torch.cuda.is_available():
                os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Hide all GPUs from PyTorch
                logger.info("üö´ CUDA disabled - GPU hidden, using CPU only")
                
                # Clear any cached CUDA state to ensure clean CPU mode
                try:
                    if hasattr(torch.cuda, 'empty_cache'):
                        torch.cuda.empty_cache()
                        logger.debug("   CUDA cache cleared")
                except Exception as e:
                    logger.debug(f"   Could not clear CUDA cache: {e}")
        elif user_device == 'cuda':
            if torch.cuda.is_available():
                logger.info("‚úÖ Forcing CUDA mode (user preference)")
                self.profile.device_type = 'cuda'
                self.profile.has_gpu = True
            else:
                logger.warning("‚ö†Ô∏è CUDA requested but not available - falling back to CPU")
                self.profile.device_type = 'cpu'
                self.profile.has_gpu = False
        elif user_device == 'mps':
            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                logger.info("‚úÖ Forcing MPS mode (user preference)")
                self.profile.device_type = 'mps'
                self.profile.has_gpu = True
            else:
                logger.warning("‚ö†Ô∏è MPS requested but not available - falling back to CPU")
                self.profile.device_type = 'cpu'
                self.profile.has_gpu = False
        else:
            logger.warning(f"‚ö†Ô∏è Unknown device '{user_device}' - using auto-detection")
    
    def check_and_optimize_device(self) -> dict:
        """
        Check system load and determine optimal device for next synthesis
        
        Returns:
            dict with device decision and reason
        """
        decision = self.intelligent_selector.get_optimal_device(self.engine.device)
        
        if decision['should_switch']:
            logger.info(f"üîÑ Smart device switch: {self.engine.device} ‚Üí {decision['device']}")
            logger.info(f"   Reason: {decision['reason']}")
            self._switch_device(decision['device'])
        
        return decision
    
    def _switch_device(self, new_device: str):
        """Switch models to new device"""
        try:
            old_device = self.engine.device
            logger.info(f"üîÑ Switching device: {old_device} ‚Üí {new_device}")
            
            # Update engine device
            self.engine.device = new_device
            
            # Move models to new device
            if hasattr(self.engine, 'llama_queue') and hasattr(self.engine.llama_queue, 'model'):
                self.engine.llama_queue.model = self.engine.llama_queue.model.to(new_device)
            if hasattr(self.engine, 'decoder_model'):
                self.engine.decoder_model = self.engine.decoder_model.to(new_device)
            
            # Clear cache from old device
            if old_device == 'cuda' and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info(f"‚úÖ Device switched successfully to {new_device}")
        except Exception as e:
            logger.error(f"‚ùå Failed to switch device: {e}")
            logger.warning(f"‚ö†Ô∏è Keeping device as {self.engine.device}")
    
    def _apply_configuration(self):
        """Apply environment configuration"""
        # Set environment variables
        os.environ['DEVICE'] = self.config.device
        os.environ['MIXED_PRECISION'] = self.config.precision
        os.environ['ENABLE_TORCH_COMPILE'] = 'true' if self.config.usetorchcompile else 'false'
        os.environ['OMP_NUM_THREADS'] = str(self.config.num_threads)
        os.environ['MKL_NUM_THREADS'] = str(self.config.num_threads)
        
        # torch.compile is only enabled on Linux/macOS/WSL2 (has Triton)
        # No special configuration needed - Triton is available by default
        
        # PyTorch settings
        torch.set_num_threads(self.config.num_threads)
        
        if self.config.device == 'cuda':
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
        
        logger.info(f"üîß Configuration applied")
    
    def _initialize_engine(self, model_path: str):
        """Initialize appropriate engine based on configuration"""
        
        # Use ONNX-optimized engine for CPU
        if self.config.useonnx and self.config.device == 'cpu':
            try:
                from universal_optimizer import UniversalFishSpeechOptimizer
                logger.info("üì¶ Loading Universal Optimizer with ONNX Runtime")
                # CRITICAL FIX: Pass device to UniversalOptimizer to ensure user preference is respected
                return UniversalFishSpeechOptimizer(model_path=model_path, device=self.config.device)
            except ImportError as e:
                logger.warning(f"Universal Optimizer not available: {e}")
                logger.info("Falling back to standard engine")
        
        # Use standard V2 engine
        from opt_engine_v2 import OptimizedFishSpeechV2
        logger.info("üì¶ Loading Standard Optimized Engine V2")
        return OptimizedFishSpeechV2(
            model_path=model_path,
            device=self.config.device,
            enable_optimizations=True,
            optimize_for_memory=(self.profile.memory_gb < 8)
        )
    
    def _truncate_text_smart(self, text: str) -> tuple[str, bool, int]:
        """
        Intelligently truncate text based on hardware constraints
        
        Returns: (truncated_text, was_truncated, original_length)
        """
        original_length = len(text)
        max_length = self.config.max_text_length
        
        if original_length <= max_length:
            return text, False, original_length
        
        # Truncate at word boundary to avoid cutting mid-word
        truncated = text[:max_length]
        last_space = truncated.rfind(' ')
        
        # If space is reasonably close (within 20% of max), cut there
        if last_space > max_length * 0.8:
            truncated = truncated[:last_space]
        
        logger.warning(
            f"[{self.profile.cpu_tier}] Text truncated: "
            f"{original_length} chars ‚Üí {len(truncated)} chars (max: {max_length})"
        )
        
        return truncated, True, original_length
    
    def tts(self, text: str, speaker_wav: Optional[str] = None, **kwargs):
        """
        Smart TTS synthesis with automatic resource monitoring and text limiting
        
        Automatically adapts if system is under pressure
        """
        # Smart text truncation based on hardware
        text, was_truncated, original_length = self._truncate_text_smart(text)
        
        if was_truncated:
            logger.info(
                f"Hardware: {self.profile.cpu_tier} | "
                f"Device: {self.profile.device_type} | "
                f"Text: {original_length} ‚Üí {len(text)} chars"
            )
        
        # Pre-check resources
        resources = self.monitor.check_resources()
        
        # Auto-adjust if needed
        adjusted_config = self.monitor.suggest_adjustment(resources, self.config)
        if adjusted_config:
            logger.warning("‚ö†Ô∏è System under pressure - auto-adjusting parameters")
            # Apply adjusted chunk length
            kwargs['chunk_length'] = adjusted_config.chunk_length
        else:
            # Use optimal chunk length
            kwargs['chunk_length'] = self.config.chunk_length
        
        # Run synthesis
        try:
            result = self.engine.tts(
                text=text,
                speaker_wav=speaker_wav,
                **kwargs
            )
            
            # Post-check resources
            post_resources = self.monitor.check_resources()
            self._log_performance(pre=resources, post=post_resources, result=result)
            
            return result
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error("üí• Out of memory! Retrying with conservative settings")
                return self._retry_with_conservative_settings(text, speaker_wav, **kwargs)
            raise
    
    def _retry_with_conservative_settings(self, text: str, speaker_wav: Optional[str], **kwargs):
        """Retry with more conservative settings after OOM"""
        kwargs['chunk_length'] = 128  # Very small chunks
        kwargs['max_new_tokens'] = 1024  # Limit output
        
        # Force garbage collection
        import gc
        gc.collect()
        if self.config.device == 'cuda':
            torch.cuda.empty_cache()
        
        return self.engine.tts(text=text, speaker_wav=speaker_wav, **kwargs)
    
    def _log_performance(self, pre: Dict, post: Dict, result: Tuple):
        """Log performance metrics"""
        audio, sr, metrics = result
        
        logger.info("üìä Performance Report:")
        logger.info(f"  Latency: {metrics['latency_ms']:.0f}ms")
        logger.info(f"  RTF: {metrics['rtf']:.2f}x")
        logger.info(f"  Memory Delta: {post['memory_available_gb'] - pre['memory_available_gb']:.2f} GB")
        if 'gpu_util' in post:
            logger.info(f"  GPU Utilization: {post['gpu_util']:.1f}%")
    
    def get_health(self) -> Dict[str, Any]:
        """Get system health with smart insights"""
        base_health = self.engine.get_health()
        resources = self.monitor.check_resources()
        
        # Add smart insights
        insights = []
        if resources['memory_percent'] > 80:
            insights.append("‚ö†Ô∏è Memory usage high - consider reducing batch size")
        if self.profile.cpu_tier == 'm1_air' and not self.profile.thermal_capable:
            insights.append("‚ö†Ô∏è M1 Air: Performance may degrade after 10-15 minutes")
        if not self.config.useonnx and self.config.device == 'cpu':
            insights.append("üí° ONNX Runtime could provide 4-5x speedup for CPU inference")
        
        base_health['smart_insights'] = insights
        base_health['current_resources'] = resources
        base_health['hardware_profile'] = {
            'tier': self.profile.cpu_tier,
            'device': self.profile.device_type,
            'thermal_monitoring': self.profile.thermal_capable
        }
        
        return base_health
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get metrics with resource monitoring"""
        base_metrics = self.engine.get_metrics()
        base_metrics['current_resources'] = self.monitor.check_resources()
        return base_metrics
    
    def clear_cache(self):
        """Clear caches"""
        self.engine.clear_cache()
    
    def cleanup(self):
        """Cleanup resources"""
        self.engine.cleanup()


# Integration example for app.py
def create_smart_backend(model_path: str = None) -> SmartAdaptiveBackend:
    """
    Factory function for creating smart backend
    
    Usage in app.py:
        from smart_backend import create_smart_backend
        
        @app.on_event("startup")
        async def startup_event():
            global engine
            model_path = os.getenv("MODEL_DIR", "checkpoints/openaudio-s1-mini")
            engine = create_smart_backend(model_path)
    """
    if model_path is None:
        model_path = os.getenv("MODEL_DIR", "checkpoints/openaudio-s1-mini")

    return SmartAdaptiveBackend(model_path=model_path)